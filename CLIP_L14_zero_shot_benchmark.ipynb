{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf1fd51",
   "metadata": {},
   "source": [
    "\n",
    "# Zero-shot Benchmark — CLIP ViT-L/14 (open_clip)\n",
    "\n",
    "Questo notebook è **pronto per la comparazione** tra modelli/approcci sul task di *architectural style classification*.\n",
    "È pensato per Colab + Google Drive e registra le metriche nel formato della tua **scorecard**.\n",
    "\n",
    "### Cosa fa\n",
    "- Carica **CLIP ViT-L/14 (openai)** via `open_clip`\n",
    "- Esegue **zero-shot** su un dataset organizzato a cartelle (`train/`, `valid/`, `test/`)\n",
    "- Calcola: **Top-1**, **Macro-F1**, **Top-5**, **Latency (ms/img)**\n",
    "- Registra anche: **GPU-h** (*0.0 in zero-shot*), **Effort umano (h)** (manuale), **Note**\n",
    "- Esporta un **CSV** `benchmark_results.csv` con una riga per questa esecuzione\n",
    "\n",
    "> Modifica solo le variabili nella sezione **Config** qui sotto (path in Drive, split, ecc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Install ===\n",
    "!pip -q install open_clip_torch scikit-learn matplotlib pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a419aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports & Config ===\n",
    "import os, time, json, math\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, top_k_accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colab + Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# --------- CONFIG (MODIFICA QUI) ---------\n",
    "DATA_ROOT = \"/content/drive/MyDrive/ArchiStyles-2\"  # cartella dataset\n",
    "SPLIT = \"test\"  # \"test\" | \"valid\" | \"train\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Modello\n",
    "MODEL_NAME = \"ViT-L-14\"   # \"ViT-L-14\" oppure \"ViT-L-14-336\"\n",
    "PRETRAINED = \"openai\"     # per L/14 e L/14-336 usare \"openai\"\n",
    "\n",
    "# Prompt templates (puoi aggiungere/variare)\n",
    "TEMPLATES = [\n",
    "    \"a photo of a {} building\",\n",
    "    \"a photograph of {} architecture\",\n",
    "    \"an example of {} architecture\",\n",
    "    \"a building in {} style\",\n",
    "]\n",
    "# Effort & note per la scorecard\n",
    "EFFORT_HOURS = 0.5   # inserisci stima ore uomo per questa run (setup+tuning)\n",
    "NOTES = \"zero-shot baseline L/14 with prompt ensemble + temperature grid\"\n",
    "# -----------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d26be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Model & Dataset ===\n",
    "# Modello + preprocess\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=MODEL_NAME,\n",
    "    pretrained=PRETRAINED\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Dataset + DataLoader\n",
    "assert os.path.isdir(DATA_ROOT), f\"DATA_ROOT non trovato: {DATA_ROOT}\"\n",
    "split_path = os.path.join(DATA_ROOT, SPLIT)\n",
    "assert os.path.isdir(split_path), f\"Split '{SPLIT}' non trovato in {DATA_ROOT}\"\n",
    "ds = ImageFolder(split_path, transform=preprocess)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "classnames = ds.classes\n",
    "num_classes = len(classnames)\n",
    "\n",
    "print(\"Classi:\", num_classes)\n",
    "print(\"Esempio classi:\", classnames[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d473c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Prompt generation & temperature scaling ===\n",
    "def make_prompts(cname):\n",
    "    return [t.format(cname) for t in TEMPLATES] + [f\"{cname} architecture\"]\n",
    "\n",
    "# Tokenize prompts (prompt ensemble)\n",
    "all_prompts, class_offsets = [], [0]\n",
    "for cname in classnames:\n",
    "    ps = make_prompts(cname)\n",
    "    all_prompts.extend(ps)\n",
    "    class_offsets.append(len(all_prompts))\n",
    "\n",
    "tokens = tokenizer(all_prompts).to(device)\n",
    "\n",
    "# Encode text once\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(tokens)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Grid semplice per temperature (moltiplica i logits)\n",
    "TEMP_GRID = [0.5, 1.0, 2.0]  # puoi estendere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da755d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Inference (with temperature search) ===\n",
    "def evaluate_with_temp(temp):\n",
    "    y_true, y_pred = [], []\n",
    "    probs_collector = []\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dl:\n",
    "            imgs = imgs.to(device, dtype=model_dtype)\n",
    "            img_features = model.encode_image(imgs)\n",
    "            img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # logit scale nativo * temperature custom\n",
    "            logit_scale = model.logit_scale.exp() * temp\n",
    "            logits = logit_scale * (img_features @ text_features.T)\n",
    "\n",
    "            # aggrega prompt per classe\n",
    "            class_logits = []\n",
    "            for i in range(num_classes):\n",
    "                s, e = class_offsets[i], class_offsets[i+1]\n",
    "                class_logits.append(logits[:, s:e].mean(dim=1))\n",
    "            class_logits = torch.stack(class_logits, dim=1)\n",
    "\n",
    "            prob = class_logits.softmax(dim=-1)\n",
    "            pred = prob.argmax(dim=-1)\n",
    "\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "            probs_collector.append(prob.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true, dtype=int)\n",
    "    y_pred = np.array(y_pred, dtype=int)\n",
    "    probs = np.concatenate(probs_collector, axis=0)\n",
    "\n",
    "    top1 = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    top5 = top_k_accuracy_score(y_true, probs, k=min(5, num_classes), labels=np.arange(num_classes))\n",
    "\n",
    "    return top1, macro_f1, top5, y_true, y_pred, probs\n",
    "\n",
    "# Cerchiamo la temperatura migliore sullo split usato (nota: per rigore, farlo su VALID)\n",
    "best = None\n",
    "for t in TEMP_GRID:\n",
    "    top1, macro_f1, top5, _, _, _ = evaluate_with_temp(t)\n",
    "    print(f\"temp={t:.2f} -> top1={top1:.4f}  macroF1={macro_f1:.4f}  top5={top5:.4f}\")\n",
    "    if best is None or top1 > best[0]:\n",
    "        best = (top1, macro_f1, top5, t)\n",
    "\n",
    "best_top1, best_macro_f1, best_top5, best_temp = best\n",
    "print(\"\\nBest temperature:\", best_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Final evaluation with best temperature & latency measurement ===\n",
    "# Recompute to get y_true/y_pred/probs with best_temp\n",
    "top1, macro_f1, top5, y_true, y_pred, probs = evaluate_with_temp(best_temp)\n",
    "\n",
    "# Latency (ms/img) misurata su alcuni batch reali\n",
    "def measure_latency(num_batches=20):\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    # warmup\n",
    "    it = iter(dl)\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            imgs, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl)\n",
    "            imgs, _ = next(it)\n",
    "        imgs = imgs.to(device, dtype=model_dtype)\n",
    "        with torch.no_grad():\n",
    "            _ = model.encode_image(imgs)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # timed\n",
    "    times = []\n",
    "    it = iter(dl)\n",
    "    for _ in range(num_batches):\n",
    "        try:\n",
    "            imgs, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(dl)\n",
    "            imgs, _ = next(it)\n",
    "        imgs = imgs.to(device, dtype=model_dtype)\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model.encode_image(imgs)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        dt = time.time() - t0\n",
    "        ms_per_img = (dt / imgs.size(0)) * 1000.0\n",
    "        times.append(ms_per_img)\n",
    "    return float(np.mean(times))\n",
    "\n",
    "latency_ms = measure_latency(num_batches=min(20, math.ceil(len(ds)/BATCH_SIZE)))\n",
    "\n",
    "# GPU-h (zero-shot): 0.0\n",
    "gpu_hours = 0.0\n",
    "\n",
    "print(f\"FINAL -> Top-1={top1:.4f} | Macro-F1={macro_f1:.4f} | Top-5={top5:.4f}\")\n",
    "print(f\"Latency: {latency_ms:.1f} ms/img | GPU-h: {gpu_hours:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8024d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Save results row to CSV ===\n",
    "row = {\n",
    "    \"Timestamp\": datetime.utcnow().isoformat(),\n",
    "    \"Model\": MODEL_NAME,\n",
    "    \"Pretrained\": PRETRAINED,\n",
    "    \"Scenario\": f\"Zero-shot ({SPLIT})\",\n",
    "    \"Techniques\": \"Prompt ensemble + temperature search\",\n",
    "    \"Top-1 Acc\": round(float(top1), 4),\n",
    "    \"Macro-F1\": round(float(macro_f1), 4),\n",
    "    \"Top-5 Acc\": round(float(best_top5), 4),\n",
    "    \"GPU-h\": gpu_hours,\n",
    "    \"Latency (ms/img)\": round(float(latency_ms), 1),\n",
    "    \"Effort umano (h)\": float(EFFORT_HOURS),\n",
    "    \"Note\": NOTES,\n",
    "    \"Classes\": int(len(classnames)),\n",
    "    \"Batch size\": int(BATCH_SIZE)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([row])\n",
    "display(df)\n",
    "\n",
    "# Salva in locale e (opzionale) in Drive\n",
    "csv_path_local = \"/content/benchmark_results.csv\"\n",
    "if os.path.exists(csv_path_local):\n",
    "    old = pd.read_csv(csv_path_local)\n",
    "    out = pd.concat([old, df], ignore_index=True)\n",
    "else:\n",
    "    out = df.copy()\n",
    "out.to_csv(csv_path_local, index=False)\n",
    "\n",
    "# Salva anche nella cartella del dataset su Drive\n",
    "csv_path_drive = os.path.join(DATA_ROOT, \"benchmark_results.csv\")\n",
    "try:\n",
    "    if os.path.exists(csv_path_drive):\n",
    "        old_d = pd.read_csv(csv_path_drive)\n",
    "        out_d = pd.concat([old_d, df], ignore_index=True)\n",
    "    else:\n",
    "        out_d = df.copy()\n",
    "    out_d.to_csv(csv_path_drive, index=False)\n",
    "    print(\"Saved CSV to:\", csv_path_local, \"and\", csv_path_drive)\n",
    "except Exception as e:\n",
    "    print(\"Saved CSV to:\", csv_path_local)\n",
    "    print(\"Drive save failed:\", e)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
